{
  
    
        "post0": {
            "title": "Time Series Analysis with Python",
            "content": "A couple years ago, I started learning how time series analysis works. It was so very interesting, but I didn&#39;t really have time after a few months to dive into it properly. That&#39;s what I&#39;ll be doing now. Before I get started, note that I&#39;ll be following Rob Hydman&#39;s excellent book Forecasting: Principles and Practice. This is an attempt to refresh my own memory, and recreate the methods in the book with Python. . Simple Methods . The basic assumption is that you&#39;re aware time series data can be encoded as a sequence: . $$ y_{1}, y_{2}, dots , y_{T} $$Where $y_{i}$ is the value of the variable we&#39;re interested in ($y$) at the time step $i$. . Average . This method is as simple as it sounds. If we want to predict the output of a given sequence at the time step $T + 1$ then we take the average of all the values which came before it: . $$ hat{y}_{T + 1} = dfrac{1}{T} sum_{i = 1}^{T} y_{i} $$ . Tip: Keep in mind that you&#8217;ll see something like this $ hat{y}_{T+1 | T}$ which simply means &quot;the predicted value of $y$ at time step $T+1$ given all the previous $T$ timesteps In R, you can use meanf. In Python (at least with the statsmodels library from what I&#39;ve seen) we don&#39;t have that. So we&#39;re going to take a sample dataset of Electrical Equipment Manufacturing as a sample dataset and implement it: . import pandas as pd import numpy as np import matplotlib.pyplot as plt df = pd.read_csv(&quot;https://raw.githubusercontent.com/selva86/datasets/master/elecequip.csv&quot;) df.date = pd.DatetimeIndex(df.date, freq=&#39;MS&#39;) df = df.set_index(&#39;date&#39;) plt.figure(figsize = (14,7)) plt.title(&quot;Electrical Equipment Manufacturing (Euro Area)&quot;) plt.plot(df.index, df.value) plt.xlim(left = min(df.index), right = max(df.index)) plt.xlabel(&quot;Date&quot;) plt.ylabel(&quot;Orders&quot;) plt.show() . . Now I&#39;ll be honest, I created my own very simple version of this for Pandas dataframes. This is not something I&#39;d suggest using for anything other than learning: . def naive_average(y, h=1): &quot;&quot;&quot; Args: y: Pandas object with DatetimeIndex h: Integer forecast horizon &quot;&quot;&quot; try: assert isinstance(y.index, pd.core.indexes.datetimes.DatetimeIndex) except: raise TypeError(&quot;y must be of type DatetimeIndex&quot;) try: assert isinstance(h, int) except: raise TypeError(&quot;h must be of type int&quot;) ts = y.copy(deep=True) if ts.index.freq is None: inf_freq = pd.infer_freq(ts.index) print( f&quot;NO FREQUENCY ASSOCIATED WITH INDEX, INFERRING FREQUENCY TO BE: {inf_freq}&quot;) ts.index = pd.DatetimeIndex(ts.index, freq=inf_freq) # Create the extended index forecast_index = pd.date_range( ts.index[-1], periods=h+1, freq=ts.index.freq, inclusive=&#39;right&#39;) # Get the average forecast_name = ts.columns[0] forecast_value = np.array([ts[f&#39;{forecast_name}&#39;].mean()]) forecast_array = np.repeat(a=forecast_value, repeats=h) # Create forecast dataframe forecast_df = pd.DataFrame( forecast_array, index=forecast_index, columns=[forecast_name]) return pd.concat([ts, forecast_df], axis=0) . Here&#39;s an example of our electricity prices forecasted out for 11 months with this method: . forecast_df = naive_average(df, h = 11) plt.figure(figsize = (14,7)) plt.title(&quot;Average Forecast&quot;) plt.plot(forecast_df[:&#39;2012-03-01&#39;].index, forecast_df[:&#39;2012-03-01&#39;].value, label = &#39;Original Series&#39;) plt.plot(forecast_df[&#39;2012-04-01&#39;:].index, forecast_df[&#39;2012-04-01&#39;:].value, color = &#39;red&#39;, label = &#39;Average Forecast&#39;) plt.legend() plt.xlim(left = min(forecast_df.index), right = max(forecast_df.index)) plt.xlabel(&quot;Date&quot;) plt.ylabel(&quot;Orders&quot;) plt.show() . NO FREQUENCY ASSOCIATED WITH INDEX, INFERRING FREQUENCY TO BE: MS . Naive . If you thought the averaging method was simple, you&#39;re in for a treat because the naive method is even simpler. If you want to predict $ hat{y}_{T+1}$ you simply set it equal to $y_{T}$. That&#39;s it. That&#39;s the entire thing. For obvious reasons I&#39;m not going to go over this: . $$ hat{y}_{T + 1 | T} = y_{T} $$Seasonal Naive . A bit more interesting is the seasonal naive method, which doesn&#39;t take the last value, but the last value of the previous season. A good example of this may be when measuring consumption of electricity, which we know is cyclic (see above). In that case, you may expect the energy consumption today to be similar to the energy consumption last year. In this case, we introduce a new variable, $m$ which is the seasonal period. We can also replace the 1 we&#39;ve been using (as a way to indicate we want to the forecast for the next time step) with $h$, making it more general. So we can define this as: . $$ hat{y}_{T+h | T} = y_{T + 1 - m(k+1)} $$Where $k$ is the integer result of $(h-1)/m$ . Now I&#39;ve got to get going, but my post tomorrow will keep going down this road. .",
            "url": "https://darpanganatra.github.io/statsblog/jupyter/python/timeseries/2022/06/03/Time-Series-Analysis-With-Python.html",
            "relUrl": "/jupyter/python/timeseries/2022/06/03/Time-Series-Analysis-With-Python.html",
            "date": " • Jun 3, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Implementing Statistics",
            "content": "Logistic Regression . What the heck is logistic regression and how do you implement it? . The simple answer is the logistic regression is a way to classify a binary target. So if you have a bunch of features in order to tell you if you&#39;re looking at a dog or a cat, you could in theory train a logistic regression algorithm on said features to tell you the next time you see an animal if the animal is a dog or a cat. . The slightly more in depth answer is that the logistic equation is defined like so: . $$ p( bar{X}) = dfrac{e^{ beta_{0} + beta_{1}X_{1} + dots + beta_{p}X_{p}}}{1 + e^{ beta_{0} + beta_{1}X_{1} + dots + beta_{p}X_{p}}} $$Where we have $p( bar{X})$ being the &quot;real&quot; function that defines the relationship between the features encoded in $ bar{X} = (X_{0}, X_{1}, dots , X_{p})$, and the coefficients $ beta_{0}, dots , beta_{p}$ are the coefficients which are &quot;learned&quot; in the machine learning process. . Now any good statistics teacher will tell you that his isn&#39;t the extent of what you should learn about logistic regression. However, I&#39;m no statistics teacher (nor do I currently have time to go through this), so what I&#39;ll say is that if you&#39;re reading this post you should also take a look at the book Introduction to Statistical Learning which has a great discussion on this topic in Chapter 4. I&#39;d also recommend this for anyone at all trying to learn statistics. . Great! Moving forward bravely to the next step, we&#39;re going to look at how to use this for a very contrived and annoying problem, which we&#39;re going to not only create ourselves, but we&#39;ll also solve ourselves. Why? Because that&#39;s the best way to learn, change my mind. So the basic idea is that we want to have certain features which will create a relatively binary output. We also want the target to rely on the features, in a binary fashion. That&#39;s simple enough: . Data Generation . import numpy as np import matplotlib.pyplot as plt def make_sample_data(minimum, maximum, resolution = 1): &quot;&quot;&quot; Creates a grid of data points with a rough boundary line separating them in 2D. Basic rule is: x - y + noise &gt;= -0.5 Args: minimum (int): Start point maximum (int): End point resolution (float): How closely you want the points to be packed Returns: xx (np.array): x coordinates yy (np.array): y coordinates target (np.array): 1/0 output array based on rule &quot;&quot;&quot; x1 = np.arange(minimum, maximum, resolution) x2 = np.arange(minimum, maximum, resolution) xx, yy = np.meshgrid(x1, x2) target = 10*(xx - yy + np.random.randint(low = -2, high = 2, size = (len(xx), len(yy))) &gt;= -0.5) return xx,yy, target x_sample, y_sample, target_sample = make_sample_data(1, 6, resolution = .07) plt.figure(figsize = (8,8)) plt.title(&quot;Sample Data&quot;) plt.scatter(x_sample, y_sample, c = target_sample) plt.xlim(left = min(x_sample[0]), right = max(x_sample[0])) plt.ylim(bottom = min(x_sample[0]), top = max(x_sample[0])) plt.xlabel(r&quot;$x_{0}$&quot;) plt.ylabel(r&quot;$x_{1}$&quot;) plt.show() . . Taking a look at the data we&#39;ve created, we can say that there&#39;s a (relatively) clear boundary present. The rule that I implemented is: . $$ f( bar{X}) = begin{cases} 1, &amp; x_{0} - x_{1} + epsilon geq -0.5 0, &amp; text{otherwise} end{cases} $$Where $ bar{X} = (x_{0}, x_{1})$ . So in the case of $ bar{X} = (2, 4)$ we have that the output will be -2, making the value of our function 1. That looks to chek out when we look at our figure above. . Model Building . Now on to the part that most of you are here for: building a logistic model using sklearn . from sklearn.linear_model import LogisticRegression import pandas as pd . . One last time, let&#39;s take a look at our data, but in a different way this time. Lets look at it in 3D, so we can see the literal sigmoidal nature of our model: . fig = plt.figure(figsize = (14,7)) ax = plt.axes(projection=&#39;3d&#39;) ax.plot_surface(x_sample, y_sample, target_sample,cmap = &#39;coolwarm&#39;, rcount=200, ccount=200) ax.set_xlabel(r&quot;$x_1$&quot;) ax.set_ylabel(r&quot;$x_2$&quot;) ax.zaxis.set_rotate_label(False) ax.set_zlabel(r&quot;$f(x_{0},x_{1})$&quot;, rotation = 0) ax.view_init(15, 70) plt.title(&quot;Training Data&quot;) plt.show() . . Note the little section in the middle where we&#39;ve added uncertainty. Everything else is relatively concrete in the output (we&#39;re keeping it simple). . Next step that we want to put our data in a nice dataframe and use LogisticRegression from sklearn.linear_model. This has some default that we&#39;re going to use. Specifically, the &quot;penalty&quot; assigned to our training is the L2 norm. I&#39;m going to make a post about the different types of penalty, but let&#39;s keep it moving for now. . df = pd.DataFrame({&#39;x1&#39;: x_sample.ravel(), &#39;x2&#39;: y_sample.ravel(), &#39;target&#39;: target_sample.ravel()}) df.head() . x1 x2 target . 0 1.00 | 1.0 | 0 | . 1 1.07 | 1.0 | 10 | . 2 1.14 | 1.0 | 10 | . 3 1.21 | 1.0 | 0 | . 4 1.28 | 1.0 | 0 | . There&#39;s our little table, and here&#39;s how we can fit the logistic regression with the defaults: . model = LogisticRegression(random_state=0) model.fit(df[[&#39;x1&#39;, &#39;x2&#39;]], df.target) . LogisticRegression(random_state=0) . Now that we&#39;ve fitted our data, we can do a few things. Yes, we can predict, but also importantly we can learn about the model itself. Remember the basic structure of the model? Well now we can determine what our model should have looked like. There are a few ways we can go about this, namely we can use the model.coef_ and model.intercept to manually calculate the output values, or we can just use model.predict. Now keep in mind I&#39;m running the prediction on the training data, mainly so I can see if the structure was captured: . df[&#39;predictions&#39;] = model.predict(df[[&#39;x1&#39;, &#39;x2&#39;]]) fig = plt.figure(figsize = (14,7)) ax = plt.axes(projection=&#39;3d&#39;) ax.plot_surface(x_sample, y_sample, df[[&#39;predictions&#39;]].to_numpy().reshape(72,72),cmap = &#39;coolwarm&#39;) ax.set_xlabel(&quot;x1&quot;) ax.set_ylabel(&quot;x2&quot;) ax.set_zlabel(&quot;pred&quot;) ax.view_init(15, 70) plt.show() . . That looks extremely similiar to the idea we had when we created the data. So on that note, I&#39;m going to leave the basics of logistic regression here. Next up we can take a closer look at prediction and accuracy of the model. .",
            "url": "https://darpanganatra.github.io/statsblog/jupyter/python/sklearn/logistic_regression/2022/06/02/Implementing-Statistics.html",
            "relUrl": "/jupyter/python/sklearn/logistic_regression/2022/06/02/Implementing-Statistics.html",
            "date": " • Jun 2, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Modelling Differential Equations in Python",
            "content": "Introduction . I&#39;ve long been interested in modelling biological systems with mathematics. This is going to be a series which goes over how to do that (for simple systems of course). . Single Differential Equation (Decay) . The first differential differential equation we&#39;re going to look at is simple exponential decay: $$ dfrac{dy}{dt} = -y $$ . The solution to this ODE is going to be relatively simple to find: . $$ begin{aligned} dfrac{dy}{dt} &amp;= -y - int dfrac{1}{y} dy &amp;= int 1 dt - ln(y) + k_{1} &amp;= t + k_{2} ln(y) &amp;= -t + k_{3} y &amp;= Ke^{-t} end{aligned} $$In this case, we have one parameter that we dont know, and that is the value of $K$. We can find this with an initial condition. So for example if $y(0) = 1$ then we&#39;d have that . $$ begin{aligned} y(0) &amp;= 1 = Ke^{-0} 1 &amp;= K end{aligned} $$Cool. Now let&#39;s see how we can build this model and plot the solution: . from scipy.integrate import solve_ivp import numpy as np import matplotlib.pyplot as plt plt.style.use(&#39;seaborn&#39;) plt.rcParams.update({&#39;font.size&#39;: 12}) . . First thing we need to work through is how to define the model. We&#39;ll be using solve_ivp rather than odeint from the scipy.integrate library. Why? Because odeint is now outdated. I&#39;ll provide a refrence to this later. . In the case of solve_ivp, we need to create our model with three things: . $t$: The time | $y$: The variable | Other arguments | At this second, we&#39;re going to ignore that last thing, because it&#39;s not necessary for us (yet). . Here&#39;s how we can model decay: . def decay_model(t,y): &quot;&quot;&quot; Simple decay model dy/dt = -y &quot;&quot;&quot; return -y . Now that we have our model created, we can feed it into solve_ivp. To do this, we need to determine a few things. Specifically: . t_span: The time over which we want to evaluate | y0: The initial value of our function | dense_output: Whether we want our output to be smooth | Now there are many other options in solve_ivp, but for now we&#39;ll go over these, just so we can get familiar. . We can set the t_span to be from $t = 0$ to $t = 10$, which we denote with a tuple. We can set our initial value to be 1 (like we showed above), and we want a dense output. Just to throw a curve ball in there (and not put ourselves to sleep) I&#39;ll solve it for a variety of different initial values: . solution_array = list() fig, ax = plt.subplots(figsize = (14,7)) for i in range(0, 5): solution = solve_ivp(fun = decay_model, t_span = [0, 6], y0 = [i], dense_output=True) ax.plot(solution.t, solution.y[0], label = fr&quot;$K = {i}$&quot;) ax.set_title(&quot;Exponential Decay Solution&quot;) ax.set_ylabel(r&#39;$Ke^{-t}$&#39;) ax.set_xlabel(r&#39;$t$&#39;) ax.set_xlim(left = 0, right = max(solution.t)) ax.set_ylim(bottom = 0) ax.legend() plt.show() . Now I realize, this was extremely exciting and you just can&#39;t wait for more. Don&#39;t worry, next time we&#39;ll implement an SIR model. And maybe mess with the populations a bit. .",
            "url": "https://darpanganatra.github.io/statsblog/jupyter/python/modelling/comp_bio/2022/05/19/Differential-Equations-1.html",
            "relUrl": "/jupyter/python/modelling/comp_bio/2022/05/19/Differential-Equations-1.html",
            "date": " • May 19, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Darpan Ganatra. I like learning anything and everything, often times all at once. I’m a data scientist currently, with a masters focused on statistics from the New Jersey Institute of Technology. .",
          "url": "https://darpanganatra.github.io/statsblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://darpanganatra.github.io/statsblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}