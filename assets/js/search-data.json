{
  
    
        "post0": {
            "title": "Linear Regression with minibatch gradient descent in TensorFlow",
            "content": "Alright, so this is going to be a quick, not very in depth approach to linear regression with TensorFlow. A lot of the functions I&#39;m using for this that have to do with TensorFlow will be explained in a later post where I describe everything as simply as possible with examples. Bear with me here. One note is that I&#39;m taking section 3.2 of the d2l.ai book and trying to make it simpler, so you may see code overlap, but hopefully this will be much more informative and easier to read (after some revisions). . import random import matplotlib.pyplot as plt import numpy as np import tensorflow as tf . . Data Generation . The simplest way to start as I always to is to generate the data. In this case, we&#39;re looking to generate the data which has two features and one output. So our true equation will be of the form: $$ y = underline{X} bar{w} + bar{b} $$ Where $ underline{X}$ is a matrix, $ bar{w}, bar{b}$ are vectors. It&#39;s important to know the dimensions of our matrix and vectors. Here we have $ underline{X}$ in the shape of (num_samples, num_features). So really that should be ($n$, 2) where $n$ is the number of samples we have. Our vectors on the other hand will be of the shape: . $w$: (num_samples, 1) | $b$: (num_samples, 1) | . To be as clear as possible, our equation looks like this: . $$ begin{bmatrix} y_1 vdots y_n end{bmatrix} = begin{bmatrix} x_{1,1} &amp; x_{1,2} &amp; dots &amp; x_{1, p} x_{2,1} &amp; x_{2,2} &amp; dots &amp; x_{2, p} vdots &amp; &amp; ddots &amp; vdots x_{n, 1} &amp; x_{n, 2} &amp; dots &amp; x_{n, p} end{bmatrix} begin{bmatrix} w_1 vdots w_p end{bmatrix} + begin{bmatrix} b_1 vdots b_n end{bmatrix} $$By the way... this isn&#39;t how you should usually look at linear regression. Usually you&#39;d include $ bar{b}$ in $ underline{X}$ and $ bar{w}$, but to be more clear we&#39;ll seperate it here. . Last thing to keep in mind is that we&#39;re adding noise (because otherwise it&#39;s not very fun!). That noise vector $ bar{ epsilon}$ will also have a shape of ($n$, 1), making our final equation: . $$ begin{bmatrix} y_1 vdots y_n end{bmatrix} = begin{bmatrix} x_{1,1} &amp; x_{1,2} &amp; dots &amp; x_{1, p} x_{2,1} &amp; x_{2,2} &amp; dots &amp; x_{2, p} vdots &amp; &amp; ddots &amp; vdots x_{n, 1} &amp; x_{n, 2} &amp; dots &amp; x_{n, p} end{bmatrix} begin{bmatrix} w_1 vdots w_p end{bmatrix} + begin{bmatrix} b_1 vdots b_n end{bmatrix} + begin{bmatrix} epsilon_1 vdots epsilon_n end{bmatrix} $$ def generate_data(w, b, num_samples): &quot;&quot;&quot; Create sample data of the form y = Xw + b + epsilon Where epsilon is random noise centered around 0, std dev 0.01 &quot;&quot;&quot; X = tf.random.normal(shape=(num_samples, w.shape[0])) w_reshaped = tf.reshape(w, shape=(-1, 1)) epsilon = tf.random.normal(shape=(num_samples, 1), stddev=0.01) y = tf.matmul(X, w_reshaped) + b + epsilon return X, y true_w = tf.constant([2, -3.4]) true_b = 4.2 features, labels = generate_data(true_w, true_b, 1000) . Data Visualization . Now we used simple parameters of $w_1 = 2$ and $w_2 = -3.4$, with $b = 4.2$. Here&#39;s what our features look like: . plt.figure(figsize=(14, 7)) plt.title(r&quot;$Xw + b + epsilon$&quot;) plt.scatter(features[:, 0], labels, label=&quot;Feature 1&quot;) plt.scatter(features[:, 1], labels, label=&quot;Feature 2&quot;) plt.xlabel(&quot;Feature Matrix&quot;) plt.ylabel(&quot;Label&quot;) plt.legend() plt.show() . . fig = plt.figure(figsize = (14,14)) ax = plt.axes(projection=&#39;3d&#39;) ax.scatter3D(features[:, 0], features[:, 1], labels, cmap = &#39;coolwarm&#39;) ax.set_xlabel(r&quot;$x_1$&quot;) ax.set_ylabel(r&quot;$x_2$&quot;) ax.zaxis.set_rotate_label(False) ax.set_zlabel(r&quot;$y$&quot;, rotation = 0) ax.view_init(20, 140) plt.title(&quot;Training Data&quot;) plt.show() . . As our second feature $X_{n, 2}$ gets more negative and our first feature $X_{n, 1}$ gets more positive, we see that the response variable $y$ increases. We can clearly see the relationship between our features, so let&#39;s see now if our model can as well! . Training &amp; (minibatch stochiastic) Gradient Descent . This section is going to cover a few things. Specifically . What the hell is gradient descent? | Why do we batch things (also what does it mean to batch things)? | Why are we using minibatch and what the hell is it anyway? | Gradient Descent . Gradient descent is an optimization algorithm. The main point of it is to find the minimum or maximum of a particular surface which is defined by a differentiable function. The obvious question to the layman at this point is going to be: what the hell do any of those words mean Darpan? Fair enough. Let&#39;s get some examples going. . A simple example of a &quot;differentiable&quot; function is the function $f(x) = x^2$. Why is it differentiable? Because you can find the effect that changing the input will have on the out for all points (technically all points in the functions domain). That&#39;s pretty much it. Here&#39;s what I mean: . $$ dfrac{d}{dx} (x^2) = 2x $$Why does that matter? Because gradient descent is an algorithm which looks at the fact that when you take the derivative of a given function (lets call that $f&#39;(x)$) and plug in values, you can see how steep the original function is. Let me show you what I mean: . x = np.linspace(-10, 10) f = x**2 plt.figure(figsize = (14,7)) plt.title(&quot;Derivative&quot;) plt.xlim(left = min(x), right = max(x)) plt.ylim(-40, 100) plt.plot(x, f, label = &#39;Original Function&#39;) plt.plot(x, [4*i - 4 for i in x], label = &#39;Rate of change at x = 2&#39;) plt.vlines(x = 2, ymin = -40, ymax = 100, linestyles=&#39;--&#39;, color=&#39;red&#39;) plt.legend() plt.show() . We see above that the steepness can be measured via finding the derivative of our original function at the point of x = 2: . $$ begin{aligned} f&#39;(x = 2) &amp;= dfrac{d}{dx}(x^2) &amp;= 2x &amp;= 2(2) f&#39;(x = 2) &amp;= 4 end{aligned} $$And that&#39;s exactly the slope of the orange line! Similarly, if we look at when $x = 5$, we end up with $f&#39;(x) = 2(5) = 10$. That means the slope is higher on the edges of the domain (our $x$ values), and lower toward the center ($x = 0$). Turns out the center is exactly where our function&#39;s minimum value is! . Now if we imagine we&#39;re starting at $x = 5$ and trying to get to the lowest possible value of our function, we know we have to go toward 0. Why is it that we know this? Because our brains are processing the shape and seeing &quot;hey that&#39;s where we&#39;re getting the lowest.&quot; . The (naive) way to get a computer to see this is through gradient descent. Here&#39;s the general idea in mathematical form: . $$ a_{n+1} = a_{n} - gamma nabla F(a_n) $$So the entire thing above says &quot;I&#39;m gonna measure the slope where I&#39;m at, and go toward the place which has the steepest slope downward.&quot; That&#39;s it, seriously. . $a_n$ is our current location (e.g. $x = 5$) | $ gamma$ is how large our steps are (e.g. should we go from 5 to 5.1 or from 5 to 5.01?) | $ nabla F(a_n)$ is the gradient (a derivative in multiple dimensions) at the place we&#39;re at (trying to figure out which way is an increasing slope and which way is a decreasing slope) | . In our case, the slope would be the error or loss between the predicted function and the actual values. . There is a lot more to say about how exactly this works and why it matters so much, but this has already been more than a bite sized post so I&#39;ll leave it at that. . Batches &#39;n Stuff . There is a small but great article here which explains it more thoroughly than I will at the moment. . Essentially batches are subsets of the dataset which you feed into the training loop. After one batch has been fed into the training loop, you use the results to update your model parameters. So if you have a dataset of size 100, you may take a batch size of 20, which would mean for each epoch, you&#39;d have to update your parameters 5 times. You can refer to this as minibatch gradient descent. We&#39;ll be using a batch size of 10. Given that our dataset has the size of 1,000 we know the number of times the parameters are updated in one epoch is: . $$ dfrac{ text{Dataset Size}}{ text{Batch Size}} = dfrac{1000}{10} = 100 text{ times} $$ def data_iter(batch_size, features, labels): &quot;&quot;&quot; Creating minibatches to use in training &quot;&quot;&quot; num_examples = len(features) indicies = list(range(num_examples)) random.shuffle(indicies) for i in range(0, num_examples, batch_size): j = tf.constant(indicies[i : min(i + batch_size, num_examples)]) yield tf.gather(features, j), tf.gather(labels, j) . The rest of the code here should be relatively clear, but if it isnt here&#39;s a brief summary: . linear_regression: Performs the actual linear regression with the tensorflow function matmul | squared_loss: Calculates the squared loss between the predicted and the actual values (MSE) | . The final training loop simply takes the values of the number of epochs (i.e. the number of times we want to complete a training iteration) and applies the tensorflow implementation of gradient to our function, with respect to our losses and the current weights and biases. Take some time to play with this if you aren&#39;t clear (and print things out!). . def linear_regression(X, w, b): return tf.matmul(X, w) + b def squared_loss(y_hat, y): return (y_hat - tf.reshape(y, y_hat.shape))**2 / 2 def sgd(params, grads, lr, batch_size): for param, grad in zip(params, grads): param.assign_sub(lr*grad/batch_size) w = tf.Variable(tf.random.normal(shape=(2, 1), mean=0, stddev=0.01), trainable=True) b = tf.Variable(tf.zeros(1), trainable=True) lr = 0.03 num_epochs = 3 net = linear_regression loss = squared_loss for epoch in range(num_epochs): for X, y in data_iter(batch_size=10, features=features, labels=labels): with tf.GradientTape() as g: l = loss(net(X, w, b), y) dw, db = g.gradient(l, [w, b]) sgd([w, b], [dw, db], lr, batch_size=10) train_l = loss(net(features, w, b), labels) print(f&quot;Epoch {epoch+1}, loss {float(tf.reduce_mean(train_l)):f}&quot;) . This is an abrupt stop, because I&#39;ve got yet another thing I&#39;d like to obsess about. Coming soon! .",
            "url": "https://darpanganatra.github.io/statsblog/jupyter/python/tensorflow/linear-regression/2022/06/13/Linear-Regression-TF.html",
            "relUrl": "/jupyter/python/tensorflow/linear-regression/2022/06/13/Linear-Regression-TF.html",
            "date": " • Jun 13, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Time Series Analysis with Python",
            "content": "A couple years ago, I started learning how time series analysis works. It was so very interesting, but I didn&#39;t really have time after a few months to dive into it properly. That&#39;s what I&#39;ll be doing now. Before I get started, note that I&#39;ll be following Rob Hydman&#39;s excellent book Forecasting: Principles and Practice. This is an attempt to refresh my own memory, and recreate the methods in the book with Python. . Simple Methods . The basic assumption is that you&#39;re aware time series data can be encoded as a sequence: . $$ y_{1}, y_{2}, dots , y_{T} $$Where $y_{i}$ is the value of the variable we&#39;re interested in ($y$) at the time step $i$. . Average . This method is as simple as it sounds. If we want to predict the output of a given sequence at the time step $T + 1$ then we take the average of all the values which came before it: . $$ hat{y}_{T + 1} = dfrac{1}{T} sum_{i = 1}^{T} y_{i} $$ . Tip: Keep in mind that you&#8217;ll see something like this $ hat{y}_{T+1 | T}$ which simply means &quot;the predicted value of $y$ at time step $T+1$ given all the previous $T$ timesteps&quot; In R, you can use meanf. In Python (at least with the statsmodels library from what I&#39;ve seen) we don&#39;t have that. So we&#39;re going to take a sample dataset of Electrical Equipment Manufacturing as a sample dataset and implement it: . import pandas as pd import numpy as np import matplotlib.pyplot as plt df = pd.read_csv(&quot;https://raw.githubusercontent.com/selva86/datasets/master/elecequip.csv&quot;) df.date = pd.DatetimeIndex(df.date, freq=&#39;MS&#39;) df = df.set_index(&#39;date&#39;) plt.figure(figsize = (14,7)) plt.title(&quot;Electrical Equipment Manufacturing (Euro Area)&quot;) plt.plot(df.index, df.value) plt.xlim(left = min(df.index), right = max(df.index)) plt.xlabel(&quot;Date&quot;) plt.ylabel(&quot;Orders&quot;) plt.show() . . Now I&#39;ll be honest, I created my own very simple version of this for Pandas dataframes. This is not something I&#39;d suggest using for anything other than learning: . def naive_average(y, h=1): &quot;&quot;&quot; Args: y: Pandas object with DatetimeIndex h: Integer forecast horizon &quot;&quot;&quot; try: assert isinstance(y.index, pd.core.indexes.datetimes.DatetimeIndex) except: raise TypeError(&quot;y must be of type DatetimeIndex&quot;) try: assert isinstance(h, int) except: raise TypeError(&quot;h must be of type int&quot;) ts = y.copy(deep=True) if ts.index.freq is None: inf_freq = pd.infer_freq(ts.index) print( f&quot;NO FREQUENCY ASSOCIATED WITH INDEX, INFERRING FREQUENCY TO BE: {inf_freq}&quot;) ts.index = pd.DatetimeIndex(ts.index, freq=inf_freq) # Create the extended index forecast_index = pd.date_range( ts.index[-1], periods=h+1, freq=ts.index.freq, inclusive=&#39;right&#39;) # Get the average forecast_name = ts.columns[0] forecast_value = np.array([ts[f&#39;{forecast_name}&#39;].mean()]) forecast_array = np.repeat(a=forecast_value, repeats=h) # Create forecast dataframe forecast_df = pd.DataFrame( forecast_array, index=forecast_index, columns=[forecast_name]) return pd.concat([ts, forecast_df], axis=0) . Here&#39;s an example of our electricity prices forecasted out for 11 months with this method: . forecast_df = naive_average(df, h = 11) plt.figure(figsize = (14,7)) plt.title(&quot;Average Forecast&quot;) plt.plot(forecast_df[:&#39;2012-03-01&#39;].index, forecast_df[:&#39;2012-03-01&#39;].value, label = &#39;Original Series&#39;) plt.plot(forecast_df[&#39;2012-04-01&#39;:].index, forecast_df[&#39;2012-04-01&#39;:].value, color = &#39;red&#39;, label = &#39;Average Forecast&#39;) plt.legend() plt.xlim(left = min(forecast_df.index), right = max(forecast_df.index)) plt.xlabel(&quot;Date&quot;) plt.ylabel(&quot;Orders&quot;) plt.show() . NO FREQUENCY ASSOCIATED WITH INDEX, INFERRING FREQUENCY TO BE: MS . Naive . If you thought the averaging method was simple, you&#39;re in for a treat because the naive method is even simpler. If you want to predict $ hat{y}_{T+1}$ you simply set it equal to $y_{T}$. That&#39;s it. That&#39;s the entire thing. For obvious reasons I&#39;m not going to go over this: . $$ hat{y}_{T + 1 | T} = y_{T} $$Seasonal Naive . A bit more interesting is the seasonal naive method, which doesn&#39;t take the last value, but the last value of the previous season. A good example of this may be when measuring consumption of electricity, which we know is cyclic (see above). In that case, you may expect the energy consumption today to be similar to the energy consumption last year. In this case, we introduce a new variable, $m$ which is the seasonal period. We can also replace the 1 we&#39;ve been using (as a way to indicate we want to the forecast for the next time step) with $h$, making it more general. So we can define this as: . $$ hat{y}_{T+h | T} = y_{T + 1 - m(k+1)} $$Where $k$ is the integer result of $(h-1)/m$ . Now I&#39;ve got to get going, but my post tomorrow will keep going down this road. .",
            "url": "https://darpanganatra.github.io/statsblog/jupyter/python/timeseries/2022/06/03/Time-Series-Analysis-With-Python.html",
            "relUrl": "/jupyter/python/timeseries/2022/06/03/Time-Series-Analysis-With-Python.html",
            "date": " • Jun 3, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Implementing Statistics",
            "content": "Logistic Regression . What the heck is logistic regression and how do you implement it? . The simple answer is the logistic regression is a way to classify a binary target. So if you have a bunch of features in order to tell you if you&#39;re looking at a dog or a cat, you could in theory train a logistic regression algorithm on said features to tell you the next time you see an animal if the animal is a dog or a cat. . The slightly more in depth answer is that the logistic equation is defined like so: . $$ p( bar{X}) = dfrac{e^{ beta_{0} + beta_{1}X_{1} + dots + beta_{p}X_{p}}}{1 + e^{ beta_{0} + beta_{1}X_{1} + dots + beta_{p}X_{p}}} $$Where we have $p( bar{X})$ being the &quot;real&quot; function that defines the relationship between the features encoded in $ bar{X} = (X_{0}, X_{1}, dots , X_{p})$, and the coefficients $ beta_{0}, dots , beta_{p}$ are the coefficients which are &quot;learned&quot; in the machine learning process. . Now any good statistics teacher will tell you that his isn&#39;t the extent of what you should learn about logistic regression. However, I&#39;m no statistics teacher (nor do I currently have time to go through this), so what I&#39;ll say is that if you&#39;re reading this post you should also take a look at the book Introduction to Statistical Learning which has a great discussion on this topic in Chapter 4. I&#39;d also recommend this for anyone at all trying to learn statistics. . Great! Moving forward bravely to the next step, we&#39;re going to look at how to use this for a very contrived and annoying problem, which we&#39;re going to not only create ourselves, but we&#39;ll also solve ourselves. Why? Because that&#39;s the best way to learn, change my mind. So the basic idea is that we want to have certain features which will create a relatively binary output. We also want the target to rely on the features, in a binary fashion. That&#39;s simple enough: . Data Generation . import numpy as np import matplotlib.pyplot as plt def make_sample_data(minimum, maximum, resolution = 1): &quot;&quot;&quot; Creates a grid of data points with a rough boundary line separating them in 2D. Basic rule is: x - y + noise &gt;= -0.5 Args: minimum (int): Start point maximum (int): End point resolution (float): How closely you want the points to be packed Returns: xx (np.array): x coordinates yy (np.array): y coordinates target (np.array): 1/0 output array based on rule &quot;&quot;&quot; x1 = np.arange(minimum, maximum, resolution) x2 = np.arange(minimum, maximum, resolution) xx, yy = np.meshgrid(x1, x2) target = 10*(xx - yy + np.random.randint(low = -2, high = 2, size = (len(xx), len(yy))) &gt;= -0.5) return xx,yy, target x_sample, y_sample, target_sample = make_sample_data(1, 6, resolution = .07) plt.figure(figsize = (8,8)) plt.title(&quot;Sample Data&quot;) plt.scatter(x_sample, y_sample, c = target_sample) plt.xlim(left = min(x_sample[0]), right = max(x_sample[0])) plt.ylim(bottom = min(x_sample[0]), top = max(x_sample[0])) plt.xlabel(r&quot;$x_{0}$&quot;) plt.ylabel(r&quot;$x_{1}$&quot;) plt.show() . . Taking a look at the data we&#39;ve created, we can say that there&#39;s a (relatively) clear boundary present. The rule that I implemented is: . $$ f( bar{X}) = begin{cases} 1, &amp; x_{0} - x_{1} + epsilon geq -0.5 0, &amp; text{otherwise} end{cases} $$Where $ bar{X} = (x_{0}, x_{1})$ . So in the case of $ bar{X} = (2, 4)$ we have that the output will be -2, making the value of our function 1. That looks to chek out when we look at our figure above. . Model Building . Now on to the part that most of you are here for: building a logistic model using sklearn . from sklearn.linear_model import LogisticRegression import pandas as pd . . One last time, let&#39;s take a look at our data, but in a different way this time. Lets look at it in 3D, so we can see the literal sigmoidal nature of our model: . fig = plt.figure(figsize = (14,7)) ax = plt.axes(projection=&#39;3d&#39;) ax.plot_surface(x_sample, y_sample, target_sample,cmap = &#39;coolwarm&#39;, rcount=200, ccount=200) ax.set_xlabel(r&quot;$x_1$&quot;) ax.set_ylabel(r&quot;$x_2$&quot;) ax.zaxis.set_rotate_label(False) ax.set_zlabel(r&quot;$f(x_{0},x_{1})$&quot;, rotation = 0) ax.view_init(15, 70) plt.title(&quot;Training Data&quot;) plt.show() . . Note the little section in the middle where we&#39;ve added uncertainty. Everything else is relatively concrete in the output (we&#39;re keeping it simple). . Next step that we want to put our data in a nice dataframe and use LogisticRegression from sklearn.linear_model. This has some default that we&#39;re going to use. Specifically, the &quot;penalty&quot; assigned to our training is the L2 norm. I&#39;m going to make a post about the different types of penalty, but let&#39;s keep it moving for now. . df = pd.DataFrame({&#39;x1&#39;: x_sample.ravel(), &#39;x2&#39;: y_sample.ravel(), &#39;target&#39;: target_sample.ravel()}) df.head() . x1 x2 target . 0 1.00 | 1.0 | 0 | . 1 1.07 | 1.0 | 10 | . 2 1.14 | 1.0 | 10 | . 3 1.21 | 1.0 | 0 | . 4 1.28 | 1.0 | 0 | . There&#39;s our little table, and here&#39;s how we can fit the logistic regression with the defaults: . model = LogisticRegression(random_state=0) model.fit(df[[&#39;x1&#39;, &#39;x2&#39;]], df.target) . LogisticRegression(random_state=0) . Now that we&#39;ve fitted our data, we can do a few things. Yes, we can predict, but also importantly we can learn about the model itself. Remember the basic structure of the model? Well now we can determine what our model should have looked like. There are a few ways we can go about this, namely we can use the model.coef_ and model.intercept to manually calculate the output values, or we can just use model.predict. Now keep in mind I&#39;m running the prediction on the training data, mainly so I can see if the structure was captured: . df[&#39;predictions&#39;] = model.predict(df[[&#39;x1&#39;, &#39;x2&#39;]]) fig = plt.figure(figsize = (14,7)) ax = plt.axes(projection=&#39;3d&#39;) ax.plot_surface(x_sample, y_sample, df[[&#39;predictions&#39;]].to_numpy().reshape(72,72),cmap = &#39;coolwarm&#39;) ax.set_xlabel(&quot;x1&quot;) ax.set_ylabel(&quot;x2&quot;) ax.set_zlabel(&quot;pred&quot;) ax.view_init(15, 70) plt.show() . . That looks extremely similiar to the idea we had when we created the data. So on that note, I&#39;m going to leave the basics of logistic regression here. Next up we can take a closer look at prediction and accuracy of the model. .",
            "url": "https://darpanganatra.github.io/statsblog/jupyter/python/sklearn/logistic_regression/2022/06/02/Implementing-Statistics.html",
            "relUrl": "/jupyter/python/sklearn/logistic_regression/2022/06/02/Implementing-Statistics.html",
            "date": " • Jun 2, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Modelling Differential Equations in Python",
            "content": "Introduction . I&#39;ve long been interested in modelling biological systems with mathematics. This is going to be a series which goes over how to do that (for simple systems of course). . Single Differential Equation (Decay) . The first differential differential equation we&#39;re going to look at is simple exponential decay: $$ dfrac{dy}{dt} = -y $$ . The solution to this ODE is going to be relatively simple to find: . $$ begin{aligned} dfrac{dy}{dt} &amp;= -y - int dfrac{1}{y} dy &amp;= int 1 dt - ln(y) + k_{1} &amp;= t + k_{2} ln(y) &amp;= -t + k_{3} y &amp;= Ke^{-t} end{aligned} $$In this case, we have one parameter that we dont know, and that is the value of $K$. We can find this with an initial condition. So for example if $y(0) = 1$ then we&#39;d have that . $$ begin{aligned} y(0) &amp;= 1 = Ke^{-0} 1 &amp;= K end{aligned} $$Cool. Now let&#39;s see how we can build this model and plot the solution: . from scipy.integrate import solve_ivp import numpy as np import matplotlib.pyplot as plt plt.style.use(&#39;seaborn&#39;) plt.rcParams.update({&#39;font.size&#39;: 12}) . . First thing we need to work through is how to define the model. We&#39;ll be using solve_ivp rather than odeint from the scipy.integrate library. Why? Because odeint is now outdated. I&#39;ll provide a refrence to this later. . In the case of solve_ivp, we need to create our model with three things: . $t$: The time | $y$: The variable | Other arguments | At this second, we&#39;re going to ignore that last thing, because it&#39;s not necessary for us (yet). . Here&#39;s how we can model decay: . def decay_model(t,y): &quot;&quot;&quot; Simple decay model dy/dt = -y &quot;&quot;&quot; return -y . Now that we have our model created, we can feed it into solve_ivp. To do this, we need to determine a few things. Specifically: . t_span: The time over which we want to evaluate | y0: The initial value of our function | dense_output: Whether we want our output to be smooth | Now there are many other options in solve_ivp, but for now we&#39;ll go over these, just so we can get familiar. . We can set the t_span to be from $t = 0$ to $t = 10$, which we denote with a tuple. We can set our initial value to be 1 (like we showed above), and we want a dense output. Just to throw a curve ball in there (and not put ourselves to sleep) I&#39;ll solve it for a variety of different initial values: . solution_array = list() fig, ax = plt.subplots(figsize = (14,7)) for i in range(0, 5): solution = solve_ivp(fun = decay_model, t_span = [0, 6], y0 = [i], dense_output=True) ax.plot(solution.t, solution.y[0], label = fr&quot;$K = {i}$&quot;) ax.set_title(&quot;Exponential Decay Solution&quot;) ax.set_ylabel(r&#39;$Ke^{-t}$&#39;) ax.set_xlabel(r&#39;$t$&#39;) ax.set_xlim(left = 0, right = max(solution.t)) ax.set_ylim(bottom = 0) ax.legend() plt.show() . Now I realize, this was extremely exciting and you just can&#39;t wait for more. Don&#39;t worry, next time we&#39;ll implement an SIR model. And maybe mess with the populations a bit. .",
            "url": "https://darpanganatra.github.io/statsblog/jupyter/python/modelling/comp_bio/2022/05/19/Differential-Equations-1.html",
            "relUrl": "/jupyter/python/modelling/comp_bio/2022/05/19/Differential-Equations-1.html",
            "date": " • May 19, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Darpan Ganatra. I like learning anything and everything, often times all at once. I’m a data scientist currently, with a masters focused on statistics from the New Jersey Institute of Technology. .",
          "url": "https://darpanganatra.github.io/statsblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://darpanganatra.github.io/statsblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}